{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Copyright (c) Microsoft Corporation. All rights reserved.</i>\n",
    "\n",
    "<i>Licensed under the MIT License.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DKN : Deep Knowledge-Aware Network for News Recommendation\n",
    "\n",
    "DKN \\[1\\] is a deep learning model which incorporates information from knowledge graph for better news recommendation. Specifically, DKN uses TransX \\[2\\] method for knowledge graph representation learning, then applies a CNN framework, named KCNN, to combine entity embedding with word embedding and generate a final embedding vector for a news article. CTR prediction is made via an attention-based neural scorer. \n",
    "\n",
    "## Properties of DKN:\n",
    "\n",
    "- DKN is a content-based deep model for CTR prediction rather than traditional ID-based collaborative filtering. \n",
    "- It makes use of knowledge entities and common sense in news content via joint learning from semantic-level and knowledge-level representations of news articles.\n",
    "- DKN uses an attention module to dynamically calculate a user's aggregated historical representaition.\n",
    "\n",
    "\n",
    "## Data format\n",
    "\n",
    "DKN takes several files as input as follows:\n",
    "\n",
    "- **training / validation / test files**: each line in these files represents one instance. Impressionid is used to evaluate performance within an impression session, so it is only used when evaluating, you can set it to 0 for training data. The format is : <br> \n",
    "`[label] [userid] [CandidateNews]%[impressionid] `<br> \n",
    "e.g., `1 train_U1 N1%0` <br> \n",
    "\n",
    "- **user history file**: each line in this file represents a users' click history. You need to set `history_size` parameter in the config file, which is the max number of user's click history we use. We will automatically keep the last `history_size` number of user click history, if user's click history is more than `history_size`, and we will automatically pad with 0 if user's click history is less than `history_size`. the format is : <br> \n",
    "`[Userid] [newsid1,newsid2...]`<br>\n",
    "e.g., `train_U1 N1,N2` <br> \n",
    "\n",
    "- **document feature file**: It contains the word and entity features for news articles. News articles are represented by aligned title words and title entities. To take a quick example, a news title may be: <i>\"Trump to deliver State of the Union address next week\"</i>, then the title words value may be `CandidateNews:34,45,334,23,12,987,3456,111,456,432` and the title entitie value may be: `entity:45,0,0,0,0,0,0,0,0,0`. Only the first value of entity vector is non-zero due to the word \"Trump\". The title value and entity value is hashed from 1 to `n` (where `n` is the number of distinct words or entities). Each feature length should be fixed at k (`doc_size` parameter), if the number of words in document is more than k, you should truncate the document to k words, and if the number of words in document is less than k, you should pad 0 to the end. \n",
    "the format is like: <br> \n",
    "`[Newsid] [w1,w2,w3...wk] [e1,e2,e3...ek]`\n",
    "\n",
    "- **word embedding/entity embedding/ context embedding files**: These are `*.npy` files of pretrained embeddings. After loading, each file is a `[n+1,k]` two-dimensional matrix, n is the number of words(or entities) of their hash dictionary, k is dimension of the embedding, note that we keep embedding 0 for zero padding. \n",
    "\n",
    "In this experiment, we used GloVe\\[4\\] vectors to initialize the word embedding. We trained entity embedding using TransE\\[2\\] on knowledge graph and context embedding is the average of the entity's neighbors in the knowledge graph.<br>\n",
    "\n",
    "## MIND dataset\n",
    "\n",
    "MIND dataset\\[3\\] is a large-scale English news dataset. It was collected from anonymized behavior logs of Microsoft News website. MIND contains 1,000,000 users, 161,013 news articles and 15,777,377 impression logs. Every news article contains rich textual content including title, abstract, body, category and entities. Each impression log contains the click events, non-clicked events and historical news click behaviors of this user before this impression.\n",
    "\n",
    "A smaller version, [MIND-small](https://azure.microsoft.com/en-us/services/open-datasets/catalog/microsoft-news-dataset/), is a small version of the MIND dataset by randomly sampling 50,000 users and their behavior logs from the MIND dataset.\n",
    "\n",
    "The datasets contains these files for both training and validation data:\n",
    "\n",
    "#### behaviors.tsv\n",
    "\n",
    "The behaviors.tsv file contains the impression logs and users' news click hostories. It has 5 columns divided by the tab symbol:\n",
    "\n",
    "+ Impression ID. The ID of an impression.\n",
    "+ User ID. The anonymous ID of a user.\n",
    "+ Time. The impression time with format \"MM/DD/YYYY HH:MM:SS AM/PM\".\n",
    "+ History. The news click history (ID list of clicked news) of this user before this impression.\n",
    "+ Impressions. List of news displayed in this impression and user's click behaviors on them (1 for click and 0 for non-click).\n",
    "\n",
    "One simple example: \n",
    "\n",
    "`1    U82271    11/11/2019 3:28:58 PM    N3130 N11621 N12917 N4574 N12140 N9748    N13390-0 N7180-0 N20785-0 N6937-0 N15776-0 N25810-0 N20820-0 N6885-0 N27294-0 N18835-0 N16945-0 N7410-0 N23967-0 N22679-0 N20532-0 N26651-0 N22078-0 N4098-0 N16473-0 N13841-0 N15660-0 N25787-0 N2315-0 N1615-0 N9087-0 N23880-0 N3600-0 N24479-0 N22882-0 N26308-0 N13594-0 N2220-0 N28356-0 N17083-0 N21415-0 N18671-0 N9440-0 N17759-0 N10861-0 N21830-0 N8064-0 N5675-0 N15037-0 N26154-0 N15368-1 N481-0 N3256-0 N20663-0 N23940-0 N7654-0 N10729-0 N7090-0 N23596-0 N15901-0 N16348-0 N13645-0 N8124-0 N20094-0 N27774-0 N23011-0 N14832-0 N15971-0 N27729-0 N2167-0 N11186-0 N18390-0 N21328-0 N10992-0 N20122-0 N1958-0 N2004-0 N26156-0 N17632-0 N26146-0 N17322-0 N18403-0 N17397-0 N18215-0 N14475-0 N9781-0 N17958-0 N3370-0 N1127-0 N15525-0 N12657-0 N10537-0 N18224-0 `\n",
    "\n",
    "#### news.tsv\n",
    "\n",
    "The news.tsv file contains the detailed information of news articles involved in the behaviors.tsv file. It has 7 columns, which are divided by the tab symbol:\n",
    "\n",
    "+ News ID\n",
    "+ Category\n",
    "+ SubCategory\n",
    "+ Title\n",
    "+ Abstract\n",
    "+ URL\n",
    "+ Title Entities (entities contained in the title of this news)\n",
    "+ Abstract Entities (entites contained in the abstract of this news)\n",
    "\n",
    "One simple example: \n",
    "\n",
    "`N46466    lifestyle    lifestyleroyals    The Brands Queen Elizabeth, Prince Charles, and Prince Philip Swear By    Shop the notebooks, jackets, and more that the royals can't live without.    https://www.msn.com/en-us/lifestyle/lifestyleroyals/the-brands-queen-elizabeth,-prince-charles,-and-prince-philip-swear-by/ss-AAGH0ET?ocid=chopendata    [{\"Label\": \"Prince Philip, Duke of Edinburgh\", \"Type\": \"P\", \"WikidataId\": \"Q80976\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [48], \"SurfaceForms\": [\"Prince Philip\"]}, {\"Label\": \"Charles, Prince of Wales\", \"Type\": \"P\", \"WikidataId\": \"Q43274\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [28], \"SurfaceForms\": [\"Prince Charles\"]}, {\"Label\": \"Elizabeth II\", \"Type\": \"P\", \"WikidataId\": \"Q9682\", \"Confidence\": 0.97, \"OccurrenceOffsets\": [11], \"SurfaceForms\": [\"Queen Elizabeth\"]}]    [] `\n",
    "\n",
    "#### entity_embedding.vec & relation_embedding.vec\n",
    "\n",
    "The entity_embedding.vec and relation_embedding.vec files contain the 100-dimensional embeddings of the entities and relations learned from the subgraph (from WikiData knowledge graph) by TransE method. In both files, the first column is the ID of entity/relation, and the other columns are the embedding vector values.\n",
    "\n",
    "One simple example: \n",
    "\n",
    "`Q42306013  0.014516 -0.106958 0.024590 ... -0.080382`\n",
    "\n",
    "\n",
    "## DKN architecture\n",
    "\n",
    "The following figure shows the architecture of DKN.\n",
    "\n",
    "![](https://recodatasets.blob.core.windows.net/images/dkn_architecture.png)\n",
    "\n",
    "DKN takes one piece of candidate news and one piece of a user’s clicked news as input. For each piece of news, a specially designed KCNN is used to process its title and generate an embedding vector. KCNN is an extension of traditional CNN that allows flexibility in incorporating symbolic knowledge from a knowledge graph into sentence representation learning. \n",
    "\n",
    "With the KCNN, we obtain a set of embedding vectors for a user’s clicked history. To get final embedding of the user with\n",
    "respect to the current candidate news, we use an attention-based method to automatically match the candidate news to each piece\n",
    "of his clicked news, and aggregate the user’s historical interests with different weights. The candidate news embedding and the user embedding are concatenated and fed into a deep neural network (DNN) to calculate the predicted probability that the user will click the candidate news."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global settings and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.6.11 | packaged by conda-forge | (default, Nov 27 2020, 18:57:37) \n",
      "[GCC 9.3.0]\n",
      "Tensorflow version: 1.15.2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "import os\n",
    "from tempfile import TemporaryDirectory\n",
    "import logging\n",
    "import papermill as pm\n",
    "import tensorflow as tf\n",
    "\n",
    "from reco_utils.dataset.download_utils import maybe_download\n",
    "from reco_utils.dataset.mind import (download_mind, \n",
    "                                     extract_mind, \n",
    "                                     read_clickhistory, \n",
    "                                     get_train_input, \n",
    "                                     get_valid_input, \n",
    "                                     get_user_history,\n",
    "                                     get_words_and_entities,\n",
    "                                     generate_embeddings) \n",
    "from reco_utils.recommender.deeprec.deeprec_utils import prepare_hparams\n",
    "from reco_utils.recommender.deeprec.models.dkn import DKN\n",
    "from reco_utils.recommender.deeprec.io.dkn_iterator import DKNTextIterator\n",
    "\n",
    "print(f\"System version: {sys.version}\")\n",
    "print(f\"Tensorflow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temp dir\n",
    "tmpdir = TemporaryDirectory()\n",
    "\n",
    "# Logger\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "handler = logging.StreamHandler(sys.stderr)\n",
    "formatter = logging.Formatter(\"%(asctime)s %(levelname)s: %(message)s\", datefmt='%I:%M:%S')\n",
    "handler.setFormatter(formatter)\n",
    "logger.handlers = [handler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Mind parameters\n",
    "MIND_SIZE = \"small\"\n",
    "\n",
    "# DKN parameters\n",
    "epochs = 10\n",
    "history_size = 50\n",
    "batch_size = 100\n",
    "\n",
    "# Paths\n",
    "data_path = os.path.join(tmpdir.name, \"mind-dkn\")\n",
    "train_file = os.path.join(data_path, \"train_mind.txt\")\n",
    "valid_file = os.path.join(data_path, \"valid_mind.txt\")\n",
    "user_history_file = os.path.join(data_path, \"user_history.txt\")\n",
    "infer_embedding_file = os.path.join(data_path, \"infer_embedding.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "In this example, let's go through a real case on how to apply DKN on a raw news dataset from the very beginning. We will download a copy of open-source MIND dataset, in its original raw format. Then we will process the raw data files into DKN's input data format, which is stated previously. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51.7k/51.7k [00:06<00:00, 8.47kKB/s]\n",
      "100%|██████████| 30.2k/30.2k [00:03<00:00, 7.60kKB/s]\n"
     ]
    }
   ],
   "source": [
    "train_zip, valid_zip = download_mind(size=MIND_SIZE, dest_path=data_path)\n",
    "train_path, valid_path = extract_mind(train_zip, valid_zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('MINDsmall_train.zip/train', 'MINDsmall_train.zip/valid')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_path, valid_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:26:05 INFO: Train file /tmp/tmpvt_ozvte/mind-dkn/train_mind.txt successfully generated\n",
      "11:26:07 INFO: Validation file /tmp/tmpvt_ozvte/mind-dkn/valid_mind.txt successfully generated\n",
      "11:26:07 INFO: User history file /tmp/tmpvt_ozvte/mind-dkn/user_history.txt successfully generated\n"
     ]
    }
   ],
   "source": [
    "train_session, train_history = read_clickhistory(train_path, \"behaviors.tsv\")\n",
    "valid_session, valid_history = read_clickhistory(valid_path, \"behaviors.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(156965, 73152)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_session), len(valid_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all([len(s[2]) != 0 for s in train_session])\n",
    "assert all([len(s[2]) != 0 for s in valid_session])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['U13740', ['N55189', 'N42782', 'N34694', 'N45794', 'N18445', 'N63302', 'N10414', 'N19347', 'N31801']]\n",
      "['N55689']\n",
      "['N35729']\n",
      "----------\n",
      "['U13740', ['N55189', 'N42782', 'N34694', 'N45794', 'N18445', 'N63302', 'N10414', 'N19347', 'N31801']]\n",
      "['N28910']\n",
      "['N20020', 'N3737', 'N43202', 'N18708', 'N30125']\n",
      "----------\n",
      "['U13740', ['N55189', 'N42782', 'N34694', 'N45794', 'N18445', 'N63302', 'N10414', 'N19347', 'N31801']]\n",
      "['N58133']\n",
      "['N13907', 'N8509', 'N47061', 'N51048', 'N22417']\n",
      "----------\n",
      "['N55189', 'N42782', 'N34694', 'N45794', 'N18445', 'N63302', 'N10414', 'N19347', 'N31801']\n"
     ]
    }
   ],
   "source": [
    "uid = 'U13740'\n",
    "sessions = [s for s in train_session if s[0] == uid]\n",
    "\n",
    "for s in sessions:\n",
    "    print(s[:2])\n",
    "    print(s[-2])\n",
    "    print(s[-1][:5])\n",
    "    print('-' * 10)\n",
    "\n",
    "print(train_history[uid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "v_sessions = [s for s in valid_session if s[0] == uid]\n",
    "\n",
    "for s in v_sessions:\n",
    "    print(s[:2])\n",
    "    print(s[-2])\n",
    "    print(s[-1][:5])\n",
    "    print('-' * 10)\n",
    "\n",
    "print(valid_history.get(uid, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 50000)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_history), len(valid_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5943, 44057, 44057)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_uids = set(train_history.keys())\n",
    "valid_uids = set(valid_history.keys())\n",
    "\n",
    "join = train_uids & valid_uids\n",
    "n_join = len(join)\n",
    "n_join, len(train_uids) - n_join, len(valid_uids) - n_join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_train_input(train_session, train_file)\n",
    "get_valid_input(valid_session, valid_file)\n",
    "get_user_history(train_history, valid_history, user_history_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 975M\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 1.1K Jan  6 11:47 dkn_MINDsmall.yaml\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 4.6M Jan  6 11:33 doc_feature.txt\n",
      "-rw-rw-r-- 1 ec2-user ec2-user  14M Jan  6 11:33 entity_embeddings_5w_100.npy\n",
      "drwxrwxr-x 2 ec2-user ec2-user 4.0K Jan  6 11:33 glove\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 823M Jan  6 11:32 glove.6B.zip\n",
      "-rw-rw-r-- 1 ec2-user ec2-user  25M Jan  6 11:26 train_mind.txt\n",
      "-rw-rw-r-- 1 ec2-user ec2-user  16M Jan  6 11:26 user_history.txt\n",
      "-rw-rw-r-- 1 ec2-user ec2-user  73M Jan  6 11:26 valid_mind.txt\n",
      "-rw-rw-r-- 1 ec2-user ec2-user  23M Jan  6 11:33 word_embeddings_5w_100.npy\n"
     ]
    }
   ],
   "source": [
    "!ls -lh {data_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 train_U13740 N55689\n",
      "0 train_U13740 N35729\n",
      "0 train_U13740 N35729\n",
      "0 train_U13740 N35729\n",
      "0 train_U13740 N35729\n",
      "1 train_U91836 N17059\n",
      "0 train_U91836 N22407\n",
      "0 train_U91836 N39317\n",
      "0 train_U91836 N33677\n",
      "0 train_U91836 N20678\n"
     ]
    }
   ],
   "source": [
    "!head -10 {train_file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 valid_U80234 N31958%0\n",
      "0 valid_U80234 N28682%0\n",
      "0 valid_U80234 N48740%0\n",
      "0 valid_U80234 N34130%0\n",
      "0 valid_U80234 N6916%0\n",
      "0 valid_U80234 N5472%0\n",
      "0 valid_U80234 N50775%0\n",
      "0 valid_U80234 N24802%0\n",
      "0 valid_U80234 N19990%0\n",
      "0 valid_U80234 N33176%0\n",
      "0 valid_U80234 N62365%0\n",
      "0 valid_U80234 N5940%0\n",
      "0 valid_U80234 N6400%0\n",
      "0 valid_U80234 N58098%0\n",
      "0 valid_U80234 N42844%0\n",
      "0 valid_U80234 N49285%0\n",
      "0 valid_U80234 N51470%0\n",
      "0 valid_U80234 N53572%0\n",
      "0 valid_U80234 N11930%0\n",
      "0 valid_U80234 N21679%0\n"
     ]
    }
   ],
   "source": [
    "!head -20 {valid_file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_U13740 N55189,N42782,N34694,N45794,N18445,N63302,N10414,N19347,N31801\n",
      "train_U91836 N31739,N6072,N63045,N23979,N35656,N43353,N8129,N1569,N17686,N13008,N21623,N6233,N14340,N48031,N62285,N44383,N23061,N16290,N6244,N45099,N58715,N59049,N7023,N50528,N42704,N46082,N8275,N15710,N59026,N8429,N30867,N56514,N19709,N31402,N31741,N54889,N9798,N62612,N2663,N16617,N6087,N13231,N63317,N61388,N59359,N51163,N30698,N34567,N54225,N32852,N55833,N64467,N3142,N13912,N29802,N44462,N29948,N4486,N5398,N14761,N47020,N65112,N31699,N37159,N61101,N14761,N3433,N10438,N61355,N21164,N22976,N2511,N48390,N58224,N48742,N35458,N24611,N37509,N21773,N41011,N19041,N25785\n",
      "train_U73700 N10732,N25792,N7563,N21087,N41087,N5445,N60384,N46616,N52500,N33164,N47289,N24233,N62058,N26378,N49475,N18870\n",
      "train_U34670 N45729,N2203,N871,N53880,N41375,N43142,N33013,N29757,N31825,N51891\n",
      "train_U8125 N10078,N56514,N14904,N33740\n"
     ]
    }
   ],
   "source": [
    "!head -5 {data_path}/user_history.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 153M\n",
      "-rw-rw-r-- 1 ec2-user ec2-user   88M Jan  6 11:25 behaviors.tsv\n",
      "-rw-rw-r-- 1 ec2-user ec2-user   25M Jan  6 11:25 entity_embedding.vec\n",
      "-rw-rw-r-- 1 ec2-user ec2-user   40M Jan  6 11:25 news.tsv\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 1021K Jan  6 11:25 relation_embedding.vec\n"
     ]
    }
   ],
   "source": [
    "!ls -lh {train_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_news = os.path.join(train_path, \"news.tsv\")\n",
    "valid_news = os.path.join(valid_path, \"news.tsv\")\n",
    "news_words, news_entities = get_words_and_entities(train_news, valid_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N55528\tlifestyle\tlifestyleroyals\tThe Brands Queen Elizabeth, Prince Charles, and Prince Philip Swear By\tShop the notebooks, jackets, and more that the royals can't live without.\thttps://assets.msn.com/labs/mind/AAGH0ET.html\t[{\"Label\": \"Prince Philip, Duke of Edinburgh\", \"Type\": \"P\", \"WikidataId\": \"Q80976\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [48], \"SurfaceForms\": [\"Prince Philip\"]}, {\"Label\": \"Charles, Prince of Wales\", \"Type\": \"P\", \"WikidataId\": \"Q43274\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [28], \"SurfaceForms\": [\"Prince Charles\"]}, {\"Label\": \"Elizabeth II\", \"Type\": \"P\", \"WikidataId\": \"Q9682\", \"Confidence\": 0.97, \"OccurrenceOffsets\": [11], \"SurfaceForms\": [\"Queen Elizabeth\"]}]\t[]\n"
     ]
    }
   ],
   "source": [
    "!head -1 {train_news}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65238, dict)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(news_words), type(news_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'brands',\n",
       " 'queen',\n",
       " 'elizabeth',\n",
       " 'prince',\n",
       " 'charles',\n",
       " 'and',\n",
       " 'prince',\n",
       " 'philip',\n",
       " 'swear',\n",
       " 'by']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_words['N55528']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65238, dict)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(news_entities), type(news_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['Prince Philip'], 'Q80976'),\n",
       " (['Prince Charles'], 'Q43274'),\n",
       " (['Queen Elizabeth'], 'Q9682')]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_entities['N55528']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "def _read_news1(filepath, news_words, news_entities, tokenizer):\n",
    "\n",
    "    with open(filepath, encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        print('line:')\n",
    "        print(line)\n",
    "        print('-' * 50)\n",
    "        splitted = line.strip(\"\\n\").split(\"\\t\")\n",
    "        print('splitted:')\n",
    "        print(splitted)\n",
    "        print('-' * 50)\n",
    "        new_id, category, subcategory, title, abstract, url, title_entities, abstract_entities = splitted\n",
    "        news_words[new_id] = tokenizer.tokenize(title.lower())\n",
    "        news_entities[new_id] = []\n",
    "        for entity in json.loads(title_entities):\n",
    "            print(\"entity:\")\n",
    "            print(entity)\n",
    "            news_entities[new_id].append(\n",
    "                (entity[\"SurfaceForms\"], entity[\"WikidataId\"])\n",
    "            )\n",
    "        break\n",
    "    return news_words, news_entities\n",
    "\n",
    "def get_words_and_entities1(train_news, valid_news):\n",
    "    \"\"\"Load words and entities\n",
    "    Args:\n",
    "        train_news (str): News train file.\n",
    "        valid_news (str): News validation file.\n",
    "    Returns: \n",
    "        dict, dict: Words and entities dictionaries.\n",
    "    \"\"\"\n",
    "    news_words = {}\n",
    "    news_entities = {}\n",
    "    tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "    news_words, news_entities = _read_news1(\n",
    "        train_news, news_words, news_entities, tokenizer\n",
    "    )\n",
    "#     news_words, news_entities = _read_news(\n",
    "#         valid_news, news_words, news_entities, tokenizer\n",
    "#     )\n",
    "    return news_words, news_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "line:\n",
      "N55528\tlifestyle\tlifestyleroyals\tThe Brands Queen Elizabeth, Prince Charles, and Prince Philip Swear By\tShop the notebooks, jackets, and more that the royals can't live without.\thttps://assets.msn.com/labs/mind/AAGH0ET.html\t[{\"Label\": \"Prince Philip, Duke of Edinburgh\", \"Type\": \"P\", \"WikidataId\": \"Q80976\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [48], \"SurfaceForms\": [\"Prince Philip\"]}, {\"Label\": \"Charles, Prince of Wales\", \"Type\": \"P\", \"WikidataId\": \"Q43274\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [28], \"SurfaceForms\": [\"Prince Charles\"]}, {\"Label\": \"Elizabeth II\", \"Type\": \"P\", \"WikidataId\": \"Q9682\", \"Confidence\": 0.97, \"OccurrenceOffsets\": [11], \"SurfaceForms\": [\"Queen Elizabeth\"]}]\t[]\n",
      "\n",
      "--------------------------------------------------\n",
      "splitted:\n",
      "['N55528', 'lifestyle', 'lifestyleroyals', 'The Brands Queen Elizabeth, Prince Charles, and Prince Philip Swear By', \"Shop the notebooks, jackets, and more that the royals can't live without.\", 'https://assets.msn.com/labs/mind/AAGH0ET.html', '[{\"Label\": \"Prince Philip, Duke of Edinburgh\", \"Type\": \"P\", \"WikidataId\": \"Q80976\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [48], \"SurfaceForms\": [\"Prince Philip\"]}, {\"Label\": \"Charles, Prince of Wales\", \"Type\": \"P\", \"WikidataId\": \"Q43274\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [28], \"SurfaceForms\": [\"Prince Charles\"]}, {\"Label\": \"Elizabeth II\", \"Type\": \"P\", \"WikidataId\": \"Q9682\", \"Confidence\": 0.97, \"OccurrenceOffsets\": [11], \"SurfaceForms\": [\"Queen Elizabeth\"]}]', '[]']\n",
      "--------------------------------------------------\n",
      "entity:\n",
      "{'Label': 'Prince Philip, Duke of Edinburgh', 'Type': 'P', 'WikidataId': 'Q80976', 'Confidence': 1.0, 'OccurrenceOffsets': [48], 'SurfaceForms': ['Prince Philip']}\n",
      "entity:\n",
      "{'Label': 'Charles, Prince of Wales', 'Type': 'P', 'WikidataId': 'Q43274', 'Confidence': 1.0, 'OccurrenceOffsets': [28], 'SurfaceForms': ['Prince Charles']}\n",
      "entity:\n",
      "{'Label': 'Elizabeth II', 'Type': 'P', 'WikidataId': 'Q9682', 'Confidence': 0.97, 'OccurrenceOffsets': [11], 'SurfaceForms': ['Queen Elizabeth']}\n"
     ]
    }
   ],
   "source": [
    "news_words1, news_entities1 = get_words_and_entities1(train_news, valid_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'N55528': ['the',\n",
       "  'brands',\n",
       "  'queen',\n",
       "  'elizabeth',\n",
       "  'prince',\n",
       "  'charles',\n",
       "  'and',\n",
       "  'prince',\n",
       "  'philip',\n",
       "  'swear',\n",
       "  'by']}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_words1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'N55528': [(['Prince Philip'], 'Q80976'),\n",
       "  (['Prince Charles'], 'Q43274'),\n",
       "  (['Queen Elizabeth'], 'Q9682')]}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_entities1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 975M\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 1.1K Jan  6 11:47 dkn_MINDsmall.yaml\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 4.6M Jan  6 11:33 doc_feature.txt\n",
      "-rw-rw-r-- 1 ec2-user ec2-user  14M Jan  6 11:33 entity_embeddings_5w_100.npy\n",
      "drwxrwxr-x 2 ec2-user ec2-user 4.0K Jan  6 11:33 glove\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 823M Jan  6 11:32 glove.6B.zip\n",
      "-rw-rw-r-- 1 ec2-user ec2-user  25M Jan  6 11:26 train_mind.txt\n",
      "-rw-rw-r-- 1 ec2-user ec2-user  16M Jan  6 11:26 user_history.txt\n",
      "-rw-rw-r-- 1 ec2-user ec2-user  73M Jan  6 11:26 valid_mind.txt\n",
      "-rw-rw-r-- 1 ec2-user ec2-user  23M Jan  6 11:33 word_embeddings_5w_100.npy\n"
     ]
    }
   ],
   "source": [
    "!ls -lh {data_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:26:22 INFO: Downloading glove...\n",
      "100%|██████████| 842k/842k [06:30<00:00, 2.16kKB/s] \n",
      "11:33:11 INFO: Loading glove with embedding dimension 100...\n",
      "11:33:24 INFO: Reading train entities...\n",
      "11:33:25 INFO: Reading valid entities...\n",
      "11:33:26 INFO: Generating word and entity indexes...\n",
      "11:33:28 INFO: Generating word embeddings...\n",
      "11:33:28 INFO: Generating entity embeddings...\n",
      "11:33:28 INFO: Saving word and entity features in /tmp/tmpvt_ozvte/mind-dkn/doc_feature.txt\n",
      "11:33:29 INFO: Saving word embeddings in /tmp/tmpvt_ozvte/mind-dkn/word_embeddings_5w_100.npy\n",
      "11:33:29 INFO: Saving word embeddings in /tmp/tmpvt_ozvte/mind-dkn/entity_embeddings_5w_100.npy\n"
     ]
    }
   ],
   "source": [
    "train_entities = os.path.join(train_path, \"entity_embedding.vec\")\n",
    "valid_entities = os.path.join(valid_path, \"entity_embedding.vec\")\n",
    "news_feature_file, word_embeddings_file, entity_embeddings_file = generate_embeddings(\n",
    "    data_path,\n",
    "    news_words,\n",
    "    news_entities,\n",
    "    train_entities,\n",
    "    valid_entities,\n",
    "    max_sentence=10,\n",
    "    word_embedding_dim=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 2.1G\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 332M Jan  6 11:32 glove.6B.100d.txt\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 662M Jan  6 11:33 glove.6B.200d.txt\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 990M Jan  6 11:33 glove.6B.300d.txt\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 164M Jan  6 11:32 glove.6B.50d.txt\n"
     ]
    }
   ],
   "source": [
    "!ls -lh {data_path}/glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 0.418 0.24968 -0.41242 0.1217 0.34527 -0.044457 -0.49688 -0.17862 -0.00066023 -0.6566 0.27843 -0.14767 -0.55677 0.14658 -0.0095095 0.011658 0.10204 -0.12792 -0.8443 -0.12181 -0.016801 -0.33279 -0.1552 -0.23131 -0.19181 -1.8823 -0.76746 0.099051 -0.42125 -0.19526 4.0071 -0.18594 -0.52287 -0.31681 0.00059213 0.0074449 0.17778 -0.15897 0.012041 -0.054223 -0.29871 -0.15749 -0.34758 -0.045637 -0.44251 0.18785 0.0027849 -0.18411 -0.11514 -0.78581\n",
      ", 0.013441 0.23682 -0.16899 0.40951 0.63812 0.47709 -0.42852 -0.55641 -0.364 -0.23938 0.13001 -0.063734 -0.39575 -0.48162 0.23291 0.090201 -0.13324 0.078639 -0.41634 -0.15428 0.10068 0.48891 0.31226 -0.1252 -0.037512 -1.5179 0.12612 -0.02442 -0.042961 -0.28351 3.5416 -0.11956 -0.014533 -0.1499 0.21864 -0.33412 -0.13872 0.31806 0.70358 0.44858 -0.080262 0.63003 0.32111 -0.46765 0.22786 0.36034 -0.37818 -0.56657 0.044691 0.30392\n",
      ". 0.15164 0.30177 -0.16763 0.17684 0.31719 0.33973 -0.43478 -0.31086 -0.44999 -0.29486 0.16608 0.11963 -0.41328 -0.42353 0.59868 0.28825 -0.11547 -0.041848 -0.67989 -0.25063 0.18472 0.086876 0.46582 0.015035 0.043474 -1.4671 -0.30384 -0.023441 0.30589 -0.21785 3.746 0.0042284 -0.18436 -0.46209 0.098329 -0.11907 0.23919 0.1161 0.41705 0.056763 -6.3681e-05 0.068987 0.087939 -0.10285 -0.13931 0.22314 -0.080803 -0.35652 0.016413 0.10216\n"
     ]
    }
   ],
   "source": [
    "!head -3 {data_path}/glove/glove.6B.50d.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/tmp/tmpvt_ozvte/mind-dkn'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'brands',\n",
       " 'queen',\n",
       " 'elizabeth',\n",
       " 'prince',\n",
       " 'charles',\n",
       " 'and',\n",
       " 'prince',\n",
       " 'philip',\n",
       " 'swear',\n",
       " 'by']"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_words['N55528']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['Prince Philip'], 'Q80976'),\n",
       " (['Prince Charles'], 'Q43274'),\n",
       " (['Queen Elizabeth'], 'Q9682')]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_entities['N55528']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MINDsmall_train.zip/train/entity_embedding.vec'"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def generate_embeddings1(\n",
    "    data_path,\n",
    "    news_words,\n",
    "    news_entities,\n",
    "    train_entities,\n",
    "    valid_entities,\n",
    "    max_sentence=10,\n",
    "    word_embedding_dim=100,\n",
    "):\n",
    "    \"\"\"Generate embeddings.\n",
    "    Args:\n",
    "        data_path (str): Data path.\n",
    "        news_words (dict): News word dictionary.\n",
    "        news_entities (dict): News entity dictionary.\n",
    "        train_entities (str): Train entity file.\n",
    "        valid_entities (str): Validation entity file.\n",
    "        max_sentence (int): Max sentence size.\n",
    "        word_embedding_dim (int): Word embedding dimension.\n",
    "    Returns:\n",
    "        str, str, str: File paths to news, word and entity embeddings.\n",
    "    \"\"\"\n",
    "    variables = {}\n",
    "    \n",
    "    embedding_dimensions = [50, 100, 200, 300]\n",
    "    if word_embedding_dim not in embedding_dimensions:\n",
    "        raise ValueError(\n",
    "            f\"Wrong embedding dimension, available options are {embedding_dimensions}\"\n",
    "        )\n",
    "\n",
    "    logger.info(\"Downloading glove...\")\n",
    "    glove_path = '/tmp/tmpvt_ozvte/mind-dkn/glove'\n",
    "\n",
    "    word_set = set()\n",
    "    word_embedding_dict = {}\n",
    "    entity_embedding_dict = {}\n",
    "\n",
    "    logger.info(f\"Loading glove with embedding dimension {word_embedding_dim}...\")\n",
    "    glove_file = \"glove.6B.\" + str(word_embedding_dim) + \"d.txt\"\n",
    "    fp_pretrain_vec = open(os.path.join(glove_path, glove_file), \"r\", encoding=\"utf-8\")\n",
    "    for i, line in enumerate(fp_pretrain_vec):\n",
    "        linesplit = line.split(\" \")\n",
    "        word_set.add(linesplit[0])\n",
    "        word_embedding_dict[linesplit[0]] = np.asarray(list(map(float, linesplit[1:])))\n",
    "        \n",
    "#         if i > 0:\n",
    "#             break\n",
    "#     print('word_set:')\n",
    "#     print(word_set)\n",
    "#     print('word_embedding_dict:')\n",
    "#     print(word_embedding_dict)\n",
    "    variables['word_set'] = word_set\n",
    "    variables['word_embedding_dict'] = word_embedding_dict\n",
    "    \n",
    "    fp_pretrain_vec.close()\n",
    "\n",
    "    logger.info(\"Reading train entities...\")\n",
    "#     print(\"train_entities:\", train_entities)\n",
    "    fp_entity_vec_train = open(train_entities, \"r\", encoding=\"utf-8\")\n",
    "    for i, line in enumerate(fp_entity_vec_train):\n",
    "        linesplit = line.split()\n",
    "        entity_embedding_dict[linesplit[0]] = np.asarray(\n",
    "            list(map(float, linesplit[1:]))\n",
    "        )\n",
    "#         if i > 0:\n",
    "#             break\n",
    "    \n",
    "    \n",
    "    fp_entity_vec_train.close()\n",
    "\n",
    "    logger.info(\"Reading valid entities...\")\n",
    "#     print(\"valid_entities:\", valid_entities)\n",
    "    fp_entity_vec_valid = open(valid_entities, \"r\", encoding=\"utf-8\")\n",
    "    for i, line in enumerate(fp_entity_vec_valid):\n",
    "        linesplit = line.split()\n",
    "        entity_embedding_dict[linesplit[0]] = np.asarray(\n",
    "            list(map(float, linesplit[1:]))\n",
    "        )\n",
    "    \n",
    "    variables['entity_embedding_dict'] = entity_embedding_dict\n",
    "    fp_entity_vec_valid.close()\n",
    "\n",
    "    logger.info(\"Generating word and entity indexes...\")\n",
    "    word_dict = {}\n",
    "    word_index = 1  # 0 is preserve for notfound entity\n",
    "    news_word_string_dict = {}\n",
    "    news_entity_string_dict = {}\n",
    "    entity2index = {}\n",
    "    entity_index = 1\n",
    "    for k, doc_id in enumerate(news_words): # { doc_id: ['the','brands','queen','elizabeth','prince'], NID2: {} ...}\n",
    "        news_word_string_dict[doc_id] = [0 for n in range(max_sentence)]\n",
    "        news_entity_string_dict[doc_id] = [0 for n in range(max_sentence)]\n",
    "        surfaceform_entityids = news_entities[doc_id]\n",
    "        for item in surfaceform_entityids:   # [(['Prince Philip'], 'Q80976'), (['Prince Charles'], 'Q43274'), (['Queen Elizabeth'], 'Q9682')]\n",
    "            if item[1] not in entity2index and item[1] in entity_embedding_dict:\n",
    "                entity2index[item[1]] = entity_index\n",
    "                entity_index = entity_index + 1\n",
    "        for i in range(len(news_words[doc_id])):  # ['the','brands','queen','elizabeth','prince']\n",
    "            if news_words[doc_id][i] in word_embedding_dict:\n",
    "                if news_words[doc_id][i] not in word_dict:\n",
    "                    word_dict[news_words[doc_id][i]] = word_index\n",
    "                    word_index = word_index + 1\n",
    "                    news_word_string_dict[doc_id][i] = word_dict[news_words[doc_id][i]]\n",
    "                else:\n",
    "                    news_word_string_dict[doc_id][i] = word_dict[news_words[doc_id][i]]\n",
    "                for item in surfaceform_entityids:\n",
    "                    for surface in item[0]:\n",
    "                        for surface_word in surface.split(\" \"):\n",
    "                            if news_words[doc_id][i] == surface_word.lower():\n",
    "                                if item[1] in entity_embedding_dict:\n",
    "                                    news_entity_string_dict[doc_id][i] = entity2index[\n",
    "                                        item[1]\n",
    "                                    ]\n",
    "            if i == max_sentence - 1:\n",
    "                break\n",
    "        \n",
    "        \n",
    "#         if k > 0:\n",
    "#             break\n",
    "    variables['word_dict'] = word_dict\n",
    "    variables['news_word_string_dict'] = news_word_string_dict\n",
    "    variables['news_entity_string_dict'] = news_entity_string_dict\n",
    "    variables['entity2index'] = entity2index\n",
    "    \n",
    "\n",
    "    logger.info(\"Generating word embeddings...\")\n",
    "    word_embeddings = np.zeros([word_index, word_embedding_dim])\n",
    "    for word in word_dict:\n",
    "        word_embeddings[word_dict[word]] = word_embedding_dict[word]\n",
    "    \n",
    "    variables['word_embeddings'] = word_embeddings\n",
    "\n",
    "\n",
    "    logger.info(\"Generating entity embeddings...\")\n",
    "    entity_embeddings = np.zeros([entity_index, word_embedding_dim])\n",
    "    for entity in entity2index:\n",
    "        entity_embeddings[entity2index[entity]] = entity_embedding_dict[entity]\n",
    "    \n",
    "    variables['entity_embeddings'] = entity_embeddings\n",
    "\n",
    "    news_feature_path = os.path.join(data_path, \"doc_feature.txt\")\n",
    "    logger.info(f\"Saving word and entity features in {news_feature_path}\")\n",
    "    fp_doc_string = open(news_feature_path, \"w\", encoding=\"utf-8\")\n",
    "    for doc_id in news_word_string_dict:\n",
    "        fp_doc_string.write(\n",
    "            doc_id\n",
    "            + \" \"\n",
    "            + \",\".join(list(map(str, news_word_string_dict[doc_id])))\n",
    "            + \" \"\n",
    "            + \",\".join(list(map(str, news_entity_string_dict[doc_id])))\n",
    "            + \"\\n\"\n",
    "        )\n",
    "\n",
    "    word_embeddings_path = os.path.join(\n",
    "        data_path, \"word_embeddings_5w_\" + str(word_embedding_dim) + \".npy\"\n",
    "    )\n",
    "    logger.info(f\"Saving word embeddings in {word_embeddings_path}\")\n",
    "    np.save(word_embeddings_path, word_embeddings)\n",
    "\n",
    "    entity_embeddings_path = os.path.join(\n",
    "        data_path, \"entity_embeddings_5w_\" + str(word_embedding_dim) + \".npy\"\n",
    "    )\n",
    "    logger.info(f\"Saving word embeddings in {entity_embeddings_path}\")\n",
    "    np.save(entity_embeddings_path, entity_embeddings)\n",
    "\n",
    "    return news_feature_path, word_embeddings_path, entity_embeddings_path, variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03:48:09 INFO: Downloading glove...\n",
      "03:48:09 INFO: Loading glove with embedding dimension 100...\n",
      "03:48:22 INFO: Reading train entities...\n",
      "03:48:23 INFO: Reading valid entities...\n",
      "03:48:24 INFO: Generating word and entity indexes...\n",
      "03:48:26 INFO: Generating word embeddings...\n",
      "03:48:27 INFO: Generating entity embeddings...\n",
      "03:48:27 INFO: Saving word and entity features in /tmp/tmpvt_ozvte/mind-dkn/doc_feature.txt\n",
      "03:48:27 INFO: Saving word embeddings in /tmp/tmpvt_ozvte/mind-dkn/word_embeddings_5w_100.npy\n",
      "03:48:27 INFO: Saving word embeddings in /tmp/tmpvt_ozvte/mind-dkn/entity_embeddings_5w_100.npy\n"
     ]
    }
   ],
   "source": [
    "news_feature_file, word_embeddings_file, entity_embeddings_file, variables = generate_embeddings1(\n",
    "    data_path,\n",
    "    news_words,\n",
    "    news_entities,\n",
    "    train_entities,\n",
    "    valid_entities,\n",
    "    max_sentence=10,\n",
    "    word_embedding_dim=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_set <class 'set'>\n",
      "word_embedding_dict <class 'dict'>\n",
      "entity_embedding_dict <class 'dict'>\n",
      "word_dict <class 'dict'>\n",
      "news_word_string_dict <class 'dict'>\n",
      "news_entity_string_dict <class 'dict'>\n",
      "entity2index <class 'dict'>\n",
      "word_embeddings <class 'numpy.ndarray'>\n",
      "entity_embeddings <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "for k, v in variables.items():\n",
    "    print(k, type(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N55528 1,2,3,4,5,6,7,5,8,9 0,0,3,3,2,2,0,2,1,0\n",
      "N19639 10,11,12,13,14,15,0,0,0,0 0,0,0,0,4,4,0,0,0,0\n",
      "N61837 1,16,17,18,19,20,21,22,1,23 0,0,0,0,0,0,0,0,0,0\n",
      "N53526 24,25,26,27,28,29,19,30,31,32 0,0,0,0,0,0,0,0,0,0\n",
      "N38324 30,33,34,35,17,36,37,38,33,39 0,0,0,0,0,5,5,0,0,0\n"
     ]
    }
   ],
   "source": [
    "!head -5 {news_feature_file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_set = variables['word_set']\n",
    "word_embedding_dict = variables['word_embedding_dict']\n",
    "entity_embedding_dict = variables['entity_embedding_dict']\n",
    "word_dict = variables['word_dict']\n",
    "news_word_string_dict = variables['news_word_string_dict']\n",
    "news_entity_string_dict = variables['news_entity_string_dict']\n",
    "entity2index = variables['entity2index']\n",
    "word_embeddings = variables['word_embeddings']\n",
    "entity_embeddings = variables['entity_embeddings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 400000, 31451, (17043, 100), 30003)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_set), len(word_embedding_dict), len(entity_embedding_dict), entity_embeddings.shape, len(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65238, 65238, 17042)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(news_words), len(news_word_string_dict), len(entity2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'brands',\n",
       " 'queen',\n",
       " 'elizabeth',\n",
       " 'prince',\n",
       " 'charles',\n",
       " 'and',\n",
       " 'prince',\n",
       " 'philip',\n",
       " 'swear',\n",
       " 'by']"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nid = 'N55528'\n",
    "news_words[nid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 5, 8, 9]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_word_string_dict[nid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 3, 3, 2, 2, 0, 2, 1, 0]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_entity_string_dict[nid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " 'Q9682',\n",
       " 'Q9682',\n",
       " 'Q43274',\n",
       " 'Q43274',\n",
       " None,\n",
       " 'Q43274',\n",
       " 'Q80976',\n",
       " None]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index2entity = {v: k for k, v in entity2index.items()}\n",
    "l = [index2entity.get(idx, None) for idx in news_entity_string_dict[nid]]\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31451, (17043, 100))"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(entity_embedding_dict), entity_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.063388, -0.181451,  0.057501, -0.091254, -0.076217, -0.052525,\n",
       "        0.0505  , -0.224871, -0.018145,  0.030722,  0.064276,  0.073063,\n",
       "        0.039489,  0.159404, -0.128784,  0.016325,  0.026797,  0.13709 ,\n",
       "        0.001849, -0.059103,  0.012091,  0.045418,  0.000591,  0.211337,\n",
       "       -0.034093, -0.074582,  0.014004, -0.099355,  0.170144,  0.109376,\n",
       "       -0.014797,  0.071172,  0.080375,  0.045563, -0.046462,  0.070108,\n",
       "        0.015413, -0.020874, -0.170324, -0.00113 ,  0.05981 ,  0.054342,\n",
       "        0.027358, -0.028995, -0.224508,  0.066281, -0.200006,  0.018186,\n",
       "        0.082396,  0.167178, -0.136239,  0.055134, -0.080195, -0.00146 ,\n",
       "        0.031078, -0.017084, -0.091176, -0.036916,  0.124642, -0.098185,\n",
       "       -0.054836,  0.152483, -0.053712,  0.092816, -0.112044, -0.072247,\n",
       "       -0.114896, -0.036541, -0.186339, -0.16061 ,  0.037342, -0.133474,\n",
       "        0.11008 ,  0.070678, -0.005586, -0.046667, -0.07201 ,  0.086424,\n",
       "        0.026165,  0.030561,  0.077888, -0.117226,  0.211597,  0.112512,\n",
       "        0.079999, -0.083398, -0.121117,  0.071751, -0.017654, -0.134979,\n",
       "       -0.051949,  0.001861,  0.124535, -0.151043, -0.263698, -0.103607,\n",
       "        0.020007, -0.101157, -0.091567,  0.035234])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eid = 'Q41'\n",
    "entity_embedding_dict[eid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.038194, -0.24487 ,  0.72812 , -0.39961 ,  0.083172,  0.043953,\n",
       "       -0.39141 ,  0.3344  , -0.57545 ,  0.087459,  0.28787 , -0.06731 ,\n",
       "        0.30906 , -0.26384 , -0.13231 , -0.20757 ,  0.33395 , -0.33848 ,\n",
       "       -0.31743 , -0.48336 ,  0.1464  , -0.37304 ,  0.34577 ,  0.052041,\n",
       "        0.44946 , -0.46971 ,  0.02628 , -0.54155 , -0.15518 , -0.14107 ,\n",
       "       -0.039722,  0.28277 ,  0.14393 ,  0.23464 , -0.31021 ,  0.086173,\n",
       "        0.20397 ,  0.52624 ,  0.17164 , -0.082378, -0.71787 , -0.41531 ,\n",
       "        0.20335 , -0.12763 ,  0.41367 ,  0.55187 ,  0.57908 , -0.33477 ,\n",
       "       -0.36559 , -0.54857 , -0.062892,  0.26584 ,  0.30205 ,  0.99775 ,\n",
       "       -0.80481 , -3.0243  ,  0.01254 , -0.36942 ,  2.2167  ,  0.72201 ,\n",
       "       -0.24978 ,  0.92136 ,  0.034514,  0.46745 ,  1.1079  , -0.19358 ,\n",
       "       -0.074575,  0.23353 , -0.052062, -0.22044 ,  0.057162, -0.15806 ,\n",
       "       -0.30798 , -0.41625 ,  0.37972 ,  0.15006 , -0.53212 , -0.2055  ,\n",
       "       -1.2526  ,  0.071624,  0.70565 ,  0.49744 , -0.42063 ,  0.26148 ,\n",
       "       -1.538   , -0.30223 , -0.073438, -0.28312 ,  0.37104 , -0.25217 ,\n",
       "        0.016215, -0.017099, -0.38984 ,  0.87424 , -0.72569 , -0.51058 ,\n",
       "       -0.52028 , -0.1459  ,  0.8278  ,  0.27062 ])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = 'the'\n",
    "word_embedding_dict[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_dict[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2.00/2.00 [00:00<00:00, 774KB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "11:47:21 WARNING: \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "11:47:21 INFO: NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "yaml_file = maybe_download(url=\"https://recodatasets.blob.core.windows.net/deeprec/deeprec/dkn/dkn_MINDsmall.yaml\", \n",
    "                           work_directory=data_path)\n",
    "hparams = prepare_hparams(yaml_file,\n",
    "                          news_feature_file=news_feature_file,\n",
    "                          user_history_file=user_history_file,\n",
    "                          wordEmb_file=word_embeddings_file,\n",
    "                          entityEmb_file=entity_embeddings_file,\n",
    "                          epochs=epochs,\n",
    "                          history_size=history_size,\n",
    "                          batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:\n",
      "  doc_size: 10\n",
      "  history_size: 50\n",
      "  word_size: 30004\n",
      "  entity_size: 17043\n",
      "  data_format: dkn\n",
      "  \n",
      "info:\n",
      "  metrics:\n",
      "  - auc\n",
      "  pairwise_metrics:\n",
      "  - group_auc\n",
      "  - mean_mrr\n",
      "  - ndcg@5;10\n",
      "  show_step: 10000\n",
      "  \n",
      "model:\n",
      "  method : classification\n",
      "  activation:\n",
      "  - sigmoid\n",
      "  attention_activation: relu\n",
      "  attention_dropout: 0.0\n",
      "  attention_layer_sizes: 100\n",
      "  dim: 100\n",
      "  use_entity: true\n",
      "  use_context: false\n",
      " \n",
      "  entity_dim: 100\n",
      "  entity_embedding_method: TransE\n",
      "  transform: true\n",
      " \n",
      "  dropout:\n",
      "  - 0.0\n",
      "  filter_sizes:\n",
      "  - 1\n",
      "  - 2\n",
      "  - 3\n",
      "  layer_sizes:\n",
      "  - 300\n",
      "  # model_type: DKN_without_context\n",
      "  model_type: dkn\n",
      "  num_filters: 100\n",
      "  infer_model_name : epoch_2\n",
      "\n",
      "train:\n",
      "  batch_size: 100\n",
      "  embed_l1: 0.000\n",
      "  embed_l2: 0.000\n",
      "  epochs: 10\n",
      "  init_method: xavier_normal\n",
      "  init_value: 0.1\n",
      "  layer_l1: 0.000\n",
      "  layer_l2: 0.000001\n",
      "  learning_rate: 0.0003\n",
      "  loss: log_loss\n",
      "  is_clip_norm: False\n",
      "  max_grad_norm: 0.2\n",
      "  optimizer: adam\n",
      "  save_model: False\n",
      "  save_epoch : 2\n",
      "  enable_BN : True\n"
     ]
    }
   ],
   "source": [
    "!cat {yaml_file}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the DKN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/reco_full/lib/python3.6/site-packages/tensorflow_core/python/ops/variables.py:2825: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:47:23 WARNING: From /home/ec2-user/anaconda3/envs/reco_full/lib/python3.6/site-packages/tensorflow_core/python/ops/variables.py:2825: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/SageMaker/workspace/recommenders/reco_utils/recommender/deeprec/models/dkn.py:308: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:47:33 WARNING: From /home/ec2-user/SageMaker/workspace/recommenders/reco_utils/recommender/deeprec/models/dkn.py:308: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/reco_full/lib/python3.6/site-packages/tensorflow_core/python/layers/normalization.py:327: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:47:33 WARNING: From /home/ec2-user/anaconda3/envs/reco_full/lib/python3.6/site-packages/tensorflow_core/python/layers/normalization.py:327: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/reco_full/lib/python3.6/site-packages/tensorflow_core/python/ops/losses/losses_impl.py:121: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:47:33 WARNING: From /home/ec2-user/anaconda3/envs/reco_full/lib/python3.6/site-packages/tensorflow_core/python/ops/losses/losses_impl.py:121: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "model = DKN(hparams, DKNTextIterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_file, valid_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the DKN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.run_eval(valid_file)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.record(\"res\", res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document embedding inference API\n",
    "\n",
    "After training, you can get document embedding through this document embedding inference API. The input file format is same with document feature file. The output file fomrat is: `[Newsid] [embedding]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.run_get_embedding(news_feature_file, infer_embedding_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results on large MIND dataset\n",
    "\n",
    "Here are performances using the large MIND dataset (1,000,000 users, 161,013 news articles and 15,777,377 impression logs). \n",
    "\n",
    "| Models | g-AUC | MRR |NDCG@5 | NDCG@10 |\n",
    "| :------| :------: | :------: | :------: | :------ |\n",
    "| LibFM | 0.5993 | 0.2823 | 0.3005 | 0.3574 |\n",
    "| Wide&Deep | 0.6216 | 0.2931 | 0.3138 | 0.3712 |\n",
    "| DKN | 0.6436 | 0.3128 | 0.3371 | 0.3908|\n",
    "\n",
    "\n",
    "Note that the results of DKN are using Microsoft recommender and the results of the first two models come from the MIND paper \\[3\\].\n",
    "We compare the results on the same test dataset. \n",
    "\n",
    "One epoch takes 6381.3s (5066.6s for training, 1314.7s for evaluating) for DKN on GPU. Hardware specification for running the large dataset: <br>\n",
    "GPU: Tesla P100-PCIE-16GB <br>\n",
    "CPU: 6 cores Intel(R) Xeon(R) CPU E5-2690 v4 @ 2.60GHz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "\\[1\\] Wang, Hongwei, et al. \"DKN: Deep Knowledge-Aware Network for News Recommendation.\" Proceedings of the 2018 World Wide Web Conference on World Wide Web. International World Wide Web Conferences Steering Committee, 2018.<br>\n",
    "\\[2\\] Knowledge Graph Embeddings including TransE, TransH, TransR and PTransE. https://github.com/thunlp/KB2E <br>\n",
    "\\[3\\] Wu, Fangzhao, et al. \"MIND: A Large-scale Dataset for News Recommendation\" Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. https://msnews.github.io/competition.html <br>\n",
    "\\[4\\] GloVe: Global Vectors for Word Representation. https://nlp.stanford.edu/projects/glove/"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "conda_reco_full",
   "language": "python",
   "name": "conda_reco_full"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
